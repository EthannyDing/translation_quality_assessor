{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as text\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D, Dense, Bidirectional, LSTM, GlobalAveragePooling1D, GlobalMaxPooling1D, Dropout, concatenate\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, recall_score, precision_score, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Importing Data\n",
      "\t5 pairs of good English-French files found.\n",
      "\t9 pairs of bad English-French files found.\n",
      "Importing Data Complete.\n",
      "\t352872 good entries\n",
      "\t417208 bad entries\n",
      "\n",
      "Importing Data\n",
      "\t1 pairs of good English-French files found.\n",
      "\t1 pairs of bad English-French files found.\n",
      "Importing Data Complete.\n",
      "\t400 good entries\n",
      "\t400 bad entries\n",
      "\n",
      "Importing Data\n",
      "\t1 pairs of good English-French files found.\n",
      "\t1 pairs of bad English-French files found.\n",
      "Importing Data Complete.\n",
      "\t400 good entries\n",
      "\t400 bad entries\n",
      "Final number of training samples: 765175\n"
     ]
    }
   ],
   "source": [
    "from preprocessing import TextPreprocessOld\n",
    "\n",
    "srcLang = \"eng\"\n",
    "tgtLang = \"fra\"\n",
    "src_vocab_size = 20000\n",
    "src_len = 150\n",
    "tgt_vocab_size = 20000\n",
    "tgt_len = 150\n",
    "train_data_dir = \"/linguistics/ethan/DL_Prototype/datasets/TQA/train\"\n",
    "test_tb_data_dir = \"/linguistics/ethan/DL_Prototype/datasets/TQA/test/TB_test\"\n",
    "test_tm_data_dir = \"/linguistics/ethan/DL_Prototype/datasets/TQA/test/TM_test\"\n",
    "\n",
    "label_class_map = {\"good\": 1, \"bad\": 0}\n",
    "tp = TextPreprocessOld(srcLang, tgtLang, src_vocab_size=src_vocab_size, src_len=src_len,\n",
    "                    tgt_vocab_size=tgt_vocab_size, tgt_len=tgt_len)\n",
    "train_src_integers, train_tgt_integers, train_labels = tp.read_dataset_from_directory(train_data_dir, label_class_map)\n",
    "test_tb_src_integers, test_tb_tgt_integers, test_tb_labels = tp.read_dataset_from_directory(test_tb_data_dir, label_class_map)\n",
    "test_tm_src_integers, test_tm_tgt_integers, test_tm_labels = tp.read_dataset_from_directory(test_tm_data_dir, label_class_map)\n",
    "\n",
    "# remove samples in training data that could exist in test data.\n",
    "test_data = set(zip(test_tm_src_integers, test_tm_tgt_integers, test_tm_labels)).union(\n",
    "            set(zip(test_tb_src_integers, test_tb_tgt_integers, test_tb_labels)))\n",
    "train_data = set(zip(train_src_integers, train_tgt_integers, train_labels))\n",
    "train_data = train_data.difference(test_data)\n",
    "\n",
    "train_src_integers = np.array([td[0] for td in train_data])\n",
    "train_tgt_integers = np.array([td[1] for td in train_data])\n",
    "train_labels = np.array([td[2] for td in train_data])\n",
    "\n",
    "print(\"Final number of training samples: {}\".format(train_src_integers.shape[0]))\n",
    "del train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = tf.keras.utils.to_categorical(train_labels, num_classes=2)\n",
    "test_tm_labels = tf.keras.utils.to_categorical(test_tm_labels, num_classes=2)\n",
    "test_tb_labels = tf.keras.utils.to_categorical(test_tb_labels, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample 100 good labeled and 100 bad labeled TM\n",
    "import pandas as pd\n",
    "output = \"/linguistics/ethan/DL_Prototype/datasets/train_sample/bad_tm.sample.xlsx\"\n",
    "good_tm = []\n",
    "bad_tm = []\n",
    "for src, tgt, label in zip(train_src_integers, train_tgt_integers, train_labels):\n",
    "    if len(bad_tm) == 100:\n",
    "        break\n",
    "    if label == 0:\n",
    "        bad_tm.append((src, tgt))\n",
    "df = pd.DataFrame(bad_tm, columns=[\"English\", \"French\"])\n",
    "df.to_excel(output, header=True, index=None)\n",
    "# train_tgt_integers[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if test data has TM which also exists in training data\n",
    "# test_data = set(zip(test_tm_src_integers, test_tm_tgt_integers, test_tm_labels))\n",
    "# train_data = set(zip(train_src_integers, train_tgt_integers, train_labels))\n",
    "# train_data = train_data.difference(test_data)\n",
    "# next(iter(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intersect_sources = [p[0] for p in intersect_data]\n",
    "# train_src_integers, train_tgt_integers, train_labels\n",
    "# np.where(test_tm_src_integers in intersect_sources, True, False)\n",
    "# a = np.array([\"1\",\"2\",\"3\",\"4\"])\n",
    "# b = np.array([\"1\",\"2\",\"5\",\"4\", \"6\"])\n",
    "# (np.in1d(a, b))\n",
    "# sum(np.in1d(test_tm_src_integers, train_src_integers))\n",
    "# new_train_src_integers, new_train_tgt_integers, new_train_labels = [], [], []\n",
    "# for s, t, l in zip(train_src_integers, train_tgt_integers, train_labels):\n",
    "#     if (s not in test_tm_src_integers) and (t not in test_tm_tgt_integers):\n",
    "#         new_train_src_integers.append(s)\n",
    "#         new_train_tgt_integers.append(t)\n",
    "#         new_train_labels.append(l)\n",
    "# len(set(zip(train_src_integers, train_tgt_integers, train_labels)))\n",
    "# train_src_integers.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor_dir = \"/linguistics/ethan/DL_Prototype/models/universal-sentence-encoder-cmlm_multilingual-preprocess_2\"\n",
    "LaBSE_dir = \"/linguistics/ethan/DL_Prototype/models/LaBSE2_encoder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text vectorization using pretrained preprocessor.\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    preprocessor = hub.KerasLayer(preprocessor_dir, trainable=False)\n",
    "    \n",
    "#     train_src_integers = preprocessor(train_src_integers)\n",
    "#     train_tgt_integers = preprocessor(train_tgt_integers)\n",
    "\n",
    "    test_src_integers = preprocessor(test_src_integers)\n",
    "    test_tgt_integers = preprocessor(test_tgt_integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_src_integers[\"input_type_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = hub.KerasLayer(LaBSE_dir, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Define TQC models with and without preprocessor layer as part of model.\"\"\"\n",
    "\n",
    "def normalize(embeds):\n",
    "    l2_norm = np.linalg.norm(embeds, 2, axis=1, keepdims=True)\n",
    "    return embeds / l2_norm\n",
    "\n",
    "def build_model(max_seq_len):\n",
    "\n",
    "    # preprocessor = hub.KerasLayer(\n",
    "    #     \"https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-preprocess/2\")\n",
    "    # encoder = hub.KerasLayer(\"https://tfhub.dev/google/LaBSE/2\", trainable=False)\n",
    "    \n",
    "    src_word_ids = Input((max_seq_len,), dtype=tf.int32)\n",
    "    src_mask_ids = Input((max_seq_len,), dtype=tf.int32)\n",
    "    src_type_ids = Input((max_seq_len,), dtype=tf.int32)\n",
    "    \n",
    "    tgt_word_ids = Input((max_seq_len,), dtype=tf.int32)\n",
    "    tgt_mask_ids = Input((max_seq_len,), dtype=tf.int32)\n",
    "    tgt_type_ids = Input((max_seq_len,), dtype=tf.int32)\n",
    "    \n",
    "    src_x = {\"input_word_ids\": src_word_ids,\n",
    "             \"input_mask\": src_mask_ids,\n",
    "             \"input_type_ids\": src_type_ids}\n",
    "    \n",
    "    tgt_x = {\"input_word_ids\": tgt_word_ids,\n",
    "             \"input_mask\": tgt_mask_ids,\n",
    "             \"input_type_ids\": tgt_type_ids}\n",
    "    \n",
    "    src_x = encoder(src_x)[\"default\"]\n",
    "    tgt_x = encoder(tgt_x)[\"default\"]\n",
    "    \n",
    "    src_x = tf.math.l2_normalize(src_x, axis=1, epsilon=1e-12, name=None)\n",
    "    tgt_x = tf.math.l2_normalize(tgt_x, axis=1, epsilon=1e-12, name=None)\n",
    "    \n",
    "    # np.matmul(english_embeds, np.transpose(italian_embeds))\n",
    "    x = tf.concat([src_x, tgt_x], axis=1)\n",
    "    #  x = GlobalMaxPooling1D(x)\n",
    "    \n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    output = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model([src_word_ids, src_mask_ids, src_type_ids, \n",
    "               tgt_word_ids, tgt_mask_ids, tgt_type_ids], output)\n",
    "    #  model = Model([src_x, tgt_x], output)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def build_model_with_preprocessor(max_seq_len, preprocessor_dir, LaBSE_dir):\n",
    "    \n",
    "    src_texts = tf.keras.layers.Input(shape=(), dtype=tf.string, name=\"input_src_text\")\n",
    "    tgt_texts = tf.keras.layers.Input(shape=(), dtype=tf.string, name=\"input_tgt_text\")\n",
    "\n",
    "    preprocessor = hub.KerasLayer(preprocessor_dir, trainable=False)\n",
    "    encoder = hub.KerasLayer(LaBSE_dir, trainable=False)\n",
    "    \n",
    "    src_x = preprocessor(src_texts)\n",
    "    tgt_x = preprocessor(tgt_texts)\n",
    "    \n",
    "    src_x = encoder(src_x)[\"default\"]\n",
    "    tgt_x = encoder(tgt_x)[\"default\"]\n",
    "    \n",
    "    src_x = tf.math.l2_normalize(src_x, axis=1, epsilon=1e-12, name=None)\n",
    "    tgt_x = tf.math.l2_normalize(tgt_x, axis=1, epsilon=1e-12, name=None)\n",
    "    \n",
    "    # np.matmul(english_embeds, np.transpose(italian_embeds))\n",
    "    x = tf.concat([src_x, tgt_x], axis=1)\n",
    "    #  x = GlobalMaxPooling1D(x)\n",
    "\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dense(8, activation='relu')(x)\n",
    "    output = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model([src_texts, tgt_texts], output)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def build_model_with_preprocessor_and_lstm(max_seq_len, preprocessor_dir, LaBSE_dir, softmax=False):\n",
    "    \"\"\"Once softmax output layer is turned on, make sure to onehot encode labeled data to shape (n, num_classes)\"\"\"\n",
    "    \n",
    "    src_texts = tf.keras.layers.Input(shape=(), dtype=tf.string, name=\"input_src_text\")\n",
    "    tgt_texts = tf.keras.layers.Input(shape=(), dtype=tf.string, name=\"input_tgt_text\")\n",
    "\n",
    "    preprocessor = hub.KerasLayer(preprocessor_dir, trainable=False)\n",
    "    encoder = hub.KerasLayer(LaBSE_dir, trainable=False)\n",
    "    \n",
    "    src_x = preprocessor(src_texts)\n",
    "    tgt_x = preprocessor(tgt_texts)\n",
    "    \n",
    "    src_x = encoder(src_x)[\"sequence_output\"]\n",
    "    tgt_x = encoder(tgt_x)[\"sequence_output\"]\n",
    "    \n",
    "    # src_x = tf.math.l2_normalize(src_x, axis=-1, epsilon=1e-12, name=None)\n",
    "    # tgt_x = tf.math.l2_normalize(tgt_x, axis=-1, epsilon=1e-12, name=None)\n",
    "    \n",
    "    # np.matmul(english_embeds, np.transpose(italian_embeds))\n",
    "    # sequence_output = tf.concat([src_x, tgt_x], axis=-1)\n",
    "    sequence_output = concatenate([src_x, tgt_x])\n",
    "    \n",
    "    # Add trainable layers on top of frozen layers to adapt the pretrained features on the new data.\n",
    "    bi_lstm = Bidirectional(LSTM(768, return_sequences=True))(sequence_output)\n",
    "    # Applying hybrid pooling approach to bi_lstm sequence output.\n",
    "    avg_pool = GlobalAveragePooling1D()(bi_lstm)\n",
    "    max_pool = GlobalMaxPooling1D()(bi_lstm)\n",
    "    concat = concatenate([avg_pool, max_pool])\n",
    "    dropout = Dropout(0.3)(concat)\n",
    "    \n",
    "    x = Dense(2048, activation=\"relu\")(dropout)\n",
    "    x = Dense(512, activation=\"relu\")(x)\n",
    "    x = Dense(8, activation=\"relu\")(x)\n",
    "    \n",
    "    if softmax:\n",
    "        output = Dense(2, activation='softmax')(x)\n",
    "        \n",
    "    else:\n",
    "        output = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    \n",
    "    model = Model([src_texts, tgt_texts], output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeds = tf.constant([[1,2,3,4], [1,2, 3, 4]], dtype=tf.float32)\n",
    "# embeds / np.linalg.norm(embeds, 2, axis=1, keepdims=True)\n",
    "# # embeds\n",
    "# tf.math.l2_normalize(\n",
    "#     embeds, axis=1, epsilon=1e-12, name=None\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Input((128,))\n",
    "output = Dense(2, activation='softmax')(x)\n",
    "# output = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(x, output)\n",
    "\n",
    "model.summary()\n",
    "x = tf.random.uniform((1,128))\n",
    "model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 / np.sqrt(30)\n",
    "# tf.keras.layers.Lambda(\n",
    "#       lambda x: tf.nn.l2_normalize(x, axis=1))(embeds)\n",
    "# tf.nn.l2_normalize(embeds, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Specify training hyperparameters\"\"\"\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "gpu_devices = get_available_gpus()\n",
    "epochs = 10\n",
    "max_seq_len = 128\n",
    "batch_size = 128\n",
    "num_samples = len(train_labels)\n",
    "steps_per_epoch = int(num_samples / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = build_model(max_seq_len)\n",
    "# model2 = build_model_with_preprocessor(max_seq_len, preprocessor_dir, LaBSE_dir)\n",
    "model3 = build_model_with_preprocessor_and_lstm(max_seq_len, preprocessor_dir, LaBSE_dir, softmax=True)\n",
    "# del model3\n",
    "# model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model2.summary()\n",
    "# mirrored_strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])\n",
    "# with mirrored_strategy.scope():\n",
    "# for d in [\"/gpu:0\", \"/gpu:1\"]:\n",
    "    \n",
    "#     with tf.device(d):\n",
    "\n",
    "#         model = build_model(max_seq_len)\n",
    "#         model.compile(optimizer=\"adam\",\n",
    "#                       loss=\"binary_crossentropy\",\n",
    "#                       metrics=[\"accuracy\"])\n",
    "#         model.fit(x=[train_src_integers[\"input_word_ids\"], train_src_integers[\"input_mask\"], train_src_integers[\"input_type_ids\"],\n",
    "#                      train_tgt_integers[\"input_word_ids\"], train_tgt_integers[\"input_mask\"], train_tgt_integers[\"input_type_ids\"]],\n",
    "#                   y=train_labels,\n",
    "#                   batch_size=batch_size,\n",
    "#                   epochs=epochs,\n",
    "#                   steps_per_epoch=steps_per_epoch,\n",
    "#                   validation_split=0.1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"Start training session.\"\"\"\n",
    "\n",
    "# Specify checkpoint saving every epoch\n",
    "checkpoint_path = \"/linguistics/ethan/DL_Prototype/models/TQA_models/Multilingual-LaBSE-Bidirectional-LSTM_ckpts/training_job_7/tqc-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, \n",
    "                                              verbose=1,\n",
    "                                              save_weights_only=False,\n",
    "                                              save_freq='epoch') # Save weights, every epoch.\n",
    "\n",
    "# Specify weights for each class, especially for imbalanced datasets.\n",
    "# weights = compute_class_weight(\"balanced\", np.unique(train_labels), train_labels)\n",
    "# class_weight = dict(zip(np.unique(train_labels), weights))\n",
    "\n",
    "# specify metrics during and after training.\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "# for d in [\"/gpu:0\", \"/gpu:1\"]:\n",
    "#     with tf.device(d):\n",
    "\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])\n",
    "with mirrored_strategy.scope():\n",
    "    #  model = build_model(max_seq_len)\n",
    "    #  model2 = build_model_with_preprocessor(max_seq_len, preprocessor_dir, LaBSE_dir)\n",
    "    model3 = build_model_with_preprocessor_and_lstm(max_seq_len, preprocessor_dir, LaBSE_dir, softmax=True)\n",
    "    model3.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                  loss=\"sparse_categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\", recall_m, precision_m])\n",
    "\n",
    "    #  model2.save_weights(checkpoint_path.format(epoch=0))\n",
    "\n",
    "    model3.fit(x=[train_src_integers, train_tgt_integers],\n",
    "               y=train_labels,\n",
    "               batch_size=batch_size,\n",
    "               epochs=epochs,\n",
    "               steps_per_epoch=steps_per_epoch,\n",
    "               callbacks=[callback],\n",
    "#                class_weight=class_weight,\n",
    "               validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "765175"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_src_integers.shape\n",
    "len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continue training from checkpoint\n",
    "# checkpoint_path = \"/linguistics/ethan/DL_Prototype/models/TQA_models/Multilingual-LaBSE-Bidirectional-LSTM_ckpts/tqc-0002.ckpt\"\n",
    "# new_model = tf.keras.models.load_model(checkpoint_path)\n",
    "# assert_allclose(model.predict(x_train),\n",
    "#                 new_model.predict(x_train),\n",
    "#                 1e-5)\n",
    "\n",
    "# fit the model\n",
    "# checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "# callbacks_list = [checkpoint]\n",
    "# new_model.fit(x=[test_tm_src_integers, test_tm_tgt_integers],\n",
    "#                y=test_tm_labels, epochs=3, batch_size=16, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_labels)\n",
    "type(train_src_integers)\n",
    "model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evalution on Test data.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions from trained model on test inputs.\n",
    "# predictions = model.predict([test_src_integers[\"input_word_ids\"], test_src_integers[\"input_mask\"], test_src_integers[\"input_type_ids\"],\n",
    "#                              test_tgt_integers[\"input_word_ids\"], test_tgt_integers[\"input_mask\"], test_tgt_integers[\"input_type_ids\"]])\n",
    "tm_predictions = model3.predict([test_tm_src_integers, test_tm_tgt_integers])\n",
    "tb_predictions = model3.predict([test_tb_src_integers, test_tb_tgt_integers])\n",
    "\n",
    "tm_pred = [1 if p > 0.5 else 0 for p in tm_predictions]\n",
    "tb_pred = [1 if p > 0.5 else 0 for p in tb_predictions]\n",
    "\n",
    "print(confusion_matrix(test_tm_labels, tm_pred, labels=[1,0]))\n",
    "print(confusion_matrix(test_tb_labels, tb_pred, labels=[1,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# threshold = 0.3\n",
    "def evaluate(predictions, true_labels):\n",
    "    records = []\n",
    "    for threshold in np.linspace(0.1, 0.9, num=9):\n",
    "        pred = [1 if p > threshold else 0 for p in predictions]\n",
    "        acc = accuracy_score(true_labels, pred)\n",
    "        rec = recall_score(true_labels, pred, labels=[1, 0])\n",
    "        pre = precision_score(true_labels, pred, labels=[1, 0])\n",
    "        f1 = f1_score(true_labels, pred, labels=[1, 0])\n",
    "        records.append((threshold, acc, rec, pre, f1))\n",
    "\n",
    "    df = pd.DataFrame(records, columns=[\"Threshold\", \"Accuracy\", \"Recall\", \"Precision\", \"F1\"])\n",
    "    return df\n",
    "\n",
    "evaluate(tm_predictions, test_tm_labels)\n",
    "# evaluate(tb_predictions, test_tb_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_model_dir = \"/linguistics/ethan/DL_Prototype/models/LaBSE2_based_tqc\"\n",
    "model.save(cls_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Load complete trained model.\"\"\"\n",
    "\n",
    "# Load trained model weights for defined model.\n",
    "cls_weights_dir = \"/linguistics/ethan/DL_Prototype/models/TQA_models/Multilingual-LaBSE-Bidirectional-LSTM_ckpts/training_job_7/tqc-0005.ckpt\"\n",
    "model3.load_weights(cls_weights_dir)\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "custom_objects={\"recall_m\": recall_m, \"precision_m\": precision_m}\n",
    "cls_model_dir = \"/linguistics/ethan/DL_Prototype/models/TQA_models/Multilingual-LaBSE-Bidirectional-LSTM_ckpts/tqc-0009.ckpt\"\n",
    "model3 = tf.keras.models.load_model(cls_model_dir, custom_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction = model3.predict()\n",
    "# output = \n",
    "# new_model3 = tf.keras.Model(model3.inputs,\n",
    "# dir(model3)\n",
    "# input_array = np.array([[1,2], [3,4]])\n",
    "# print(input_array.shape)\n",
    "# dense_output = Dense(1, activation='sigmoid')(input_array)\n",
    "# print(dense_output) \n",
    "# tf.where(dense_output > 0.5, 1, 0)\n",
    "# dense_output = np.array([[0.2], [0.4], [0.6]])\n",
    "# np.where(dense_output > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "accuracy_fn = tf.keras.metrics.BinaryAccuracy()\n",
    "logits = np.array([[0.4, 0.6], [0.2, 0.8]])\n",
    "targets = np.array([[1, 0], [0, 1]])\n",
    "print(loss_fn(targets, logits, None))\n",
    "print(accuracy_fn(targets, logits, None))\n",
    "print(tf.nn.softmax(logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify1(text_pairs, threshold=0.5):\n",
    "    \n",
    "    src_integers = preprocessor([p[0] for p in text_pairs])\n",
    "    tgt_integers = preprocessor([p[1] for p in text_pairs])\n",
    "    \n",
    "    predictions = model.predict([src_integers[\"input_word_ids\"], src_integers[\"input_mask\"], src_integers[\"input_type_ids\"],\n",
    "                                 tgt_integers[\"input_word_ids\"], tgt_integers[\"input_mask\"], tgt_integers[\"input_type_ids\"]])\n",
    "    \n",
    "    return [1 if p > threshold else 0 for p in predictions]\n",
    "\n",
    "def classify2(text_pairs, threshold=0.5):\n",
    "    \n",
    "    return [1 if p > threshold else 0 for p in model3.predict(text_pairs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pairs = [(\"Poly Property Group Company Limited\", \n",
    "               \"Poly Property Group Company Limited\"),\n",
    "              \n",
    "              (\"Amazon also indicated that it has moved its AI plans from hype to reality.\", \n",
    "               \"Amazon a aussi indiqué que ses projets d’IA, jusqu’alors des mythes, étaient devenus réalité.\"),\n",
    "              \n",
    "              (\"A regulation may not be made before the earliest of\", \n",
    "               \"Le règlement ne peut être pris avant le premier en date des jours suivants\"),\n",
    "             \n",
    "              (\"Nancy J. Kyle is a vice chairman and director of CGTC.\",\n",
    "               \"Nancy J. Kyle est vice-présidente du conseil d’administration et\")]\n",
    "\n",
    "classify(text_pairs, threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_texts = tf.constant([ \"Poly Property Group Company Limited\",\n",
    "                          \"Amazon also indicated that it has moved its AI plans from hype to reality.\", \n",
    "                          \"A regulation may not be made before the earliest of\",\n",
    "                          \"Nancy J. Kyle is a vice chairman and director of CGTC.\",\n",
    "                          \"You can copy-paste\"])\n",
    "\n",
    "tgt_texts = tf.constant([\"Poly Property Group Company Limited\", \n",
    "                         \"Amazon a aussi indiqué que ses projets d’IA, jusqu’alors des mythes, étaient devenus réalité.\",\n",
    "                         \"Le règlement ne peut être pris avant le premier en date des jours suivants\",\n",
    "                         \"Nancy J. Kyle est vice-présidente du conseil d’administration et\",\n",
    "                         \"Il est possible de copier-coller\"])\n",
    "\n",
    "[\"Poly Property Group Company Limited\", \"Amazon a aussi indiqué que ses projets d’IA, jusqu’alors des mythes, étaient devenus réalité.\", \"Le règlement ne peut être pris avant le premier en date des jours suivants\", \"Nancy J. Kyle est vice-présidente du conseil d’administration et\", \"Il est possible de copier-coller\"]\n",
    "classify2([src_texts, tgt_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model2.predict([src_texts, tgt_texts])\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model3, \"/linguistics/ethan/DL_Prototype/models/TQA_models/Multilingual-LaBSE-Bidirectional-LSTM_ckpts/training_job_7/LaBSE_bi-LSTM_based.softmax.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
