{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "import re, string, os\n",
    "import random\n",
    "\n",
    "# Force Keras to use CPU.\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "def read_text(file):\n",
    "    with open(file, \"r\") as f:\n",
    "        lines = f.read().splitlines()\n",
    "\n",
    "    return lines\n",
    "\n",
    "\n",
    "class TextPreprocess(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, srcLang=\"eng\", tgtLang=\"fra\",\n",
    "                       src_vocab_size=20000, src_len=200,\n",
    "                       tgt_vocab_size=20000, tgt_len=200):\n",
    "        super(TextPreprocess, self).__init__()\n",
    "        # self.batch_size = batch_size\n",
    "        self.srcLang = srcLang\n",
    "        self.tgtLang = tgtLang\n",
    "        self.src_vocab_size = src_vocab_size\n",
    "        self.src_len = src_len\n",
    "        self.tgt_vocab_size = tgt_vocab_size\n",
    "        self.tgt_len = tgt_len\n",
    "        self.src_text_vectorizer = TextVectorization(standardize=self.custom_standardization,\n",
    "                                                     max_tokens=self.src_vocab_size,\n",
    "                                                     output_mode=\"int\",\n",
    "                                                     output_sequence_length=self.src_len)\n",
    "        self.tgt_text_vectorizer = TextVectorization(standardize=self.custom_standardization,\n",
    "                                                     max_tokens=self.tgt_vocab_size,\n",
    "                                                     output_mode=\"int\",\n",
    "                                                     output_sequence_length=self.tgt_len)\n",
    "\n",
    "    def suffle_data(self, src_lines, tgt_lines, labels, random_state=1):\n",
    "        \"\"\"Shuffle data mainly for training data.\"\"\"\n",
    "        random.seed(random_state)\n",
    "        random.shuffle(src_lines)\n",
    "        random.seed(random_state)\n",
    "        random.shuffle(tgt_lines)\n",
    "        random.seed(random_state)\n",
    "        random.shuffle(labels)\n",
    "\n",
    "        return src_lines, tgt_lines, labels\n",
    "\n",
    "    def read_dataset_from_directory(self, data_dir, label_class_map, shuffle=True):\n",
    "        \"\"\"Read TQA data from directory where the label is indicated in file name.\"\"\"\n",
    "        print(\"\\nImporting Data\")\n",
    "        files = os.listdir(data_dir)\n",
    "        good_src_prefix = [file.replace(\".good.\" + self.srcLang, \"\") for file in files if file.endswith(\".good.\" + self.srcLang)]\n",
    "        good_tgt_prefix = [file.replace(\".good.\" + self.tgtLang, \"\") for file in files if file.endswith(\".good.\" + self.tgtLang)]\n",
    "        bad_src_prefix = [file.replace(\".bad.\" + self.srcLang, \"\") for file in files if file.endswith(\".bad.\" + self.srcLang)]\n",
    "        bad_tgt_prefix = [file.replace(\".bad.\" + self.tgtLang, \"\") for file in files if file.endswith(\".bad.\" + self.tgtLang)]\n",
    "\n",
    "        assert set(good_src_prefix) == set(good_tgt_prefix), \\\n",
    "            \"The number of good English and French file pairs not equal.\"\n",
    "\n",
    "        assert set(bad_src_prefix) == set(bad_tgt_prefix), \\\n",
    "            \"The number of bad English and French file pairs not equal.\"\n",
    "\n",
    "        print(\"\\t{} pairs of good English-French files found.\".format(len(good_src_prefix)))\n",
    "        print(\"\\t{} pairs of bad English-French files found.\".format(len(bad_src_prefix)))\n",
    "\n",
    "        all_prefix_by_class = [prefix + \".good\" for prefix in good_src_prefix] + \\\n",
    "                              [prefix + \".bad\" for prefix in bad_src_prefix]\n",
    "        src_lines = []\n",
    "        tgt_lines = []\n",
    "        labels = []\n",
    "\n",
    "        for prefix in all_prefix_by_class:\n",
    "\n",
    "            label = prefix.split(\".\")[-1]\n",
    "            en_path = os.path.join(data_dir, prefix + \".\" + self.srcLang)\n",
    "            fr_path = os.path.join(data_dir, prefix + \".\" + self.tgtLang)\n",
    "            g_en_lines = read_text(en_path)\n",
    "            g_fr_lines = read_text(fr_path)\n",
    "\n",
    "            if len(g_en_lines) == len(g_fr_lines):\n",
    "                class_num = label_class_map.get(label)\n",
    "                src_lines += g_en_lines\n",
    "                tgt_lines += g_fr_lines\n",
    "                labels += [class_num] * len(g_en_lines)\n",
    "\n",
    "        if shuffle:\n",
    "            src_lines, tgt_lines, labels = self.suffle_data(src_lines, tgt_lines, labels)\n",
    "        counter = Counter(labels)\n",
    "        print(\"Importing Data Complete.\")\n",
    "        print(\"\\t{} good entries\".format(counter[label_class_map[\"good\"]]))\n",
    "        print(\"\\t{} bad entries\".format(counter[label_class_map[\"bad\"]]))\n",
    "\n",
    "        src_lines = np.array(src_lines).reshape(len(src_lines), 1)\n",
    "        tgt_lines = np.array(tgt_lines).reshape(len(tgt_lines), 1)\n",
    "        labels = np.array(labels).reshape(len(labels), 1)\n",
    "\n",
    "        return src_lines, tgt_lines, labels\n",
    "\n",
    "    def custom_standardization(self, input_data):\n",
    "        \"\"\"Customized manipulations on raw text.\"\"\"\n",
    "        lowercase = tf.strings.lower(input_data)\n",
    "        stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
    "        return tf.strings.regex_replace(\n",
    "            stripped_html, \"[%s]\" % re.escape(string.punctuation), \"\"\n",
    "        )\n",
    "\n",
    "    def create_integer_ds(self, src_text, tgt_text):\n",
    "        # src_text, tgt_text = samples\n",
    "\n",
    "        src_int_samples = self.src_text_vectorizer(src_text)\n",
    "        tgt_int_samples = self.tgt_text_vectorizer(tgt_text)\n",
    "\n",
    "        return src_int_samples, tgt_int_samples\n",
    "\n",
    "    def create_datasets(self, data_dir, label_class_map, mode=\"train\", batch_size=32):\n",
    "        \"\"\"Create datasets used for training and testing from local files\"\"\"\n",
    "\n",
    "        src_lines, tgt_lines, labels = self.read_dataset_from_directory(data_dir, label_class_map, shuffle=True)\n",
    "\n",
    "        # labels = tf.data.Dataset.from_tensor_slices((labels)).batch(batch_size)\n",
    "        if mode == \"train\":\n",
    "\n",
    "            # train_src_text = dataset.map(lambda src, tgt: src)\n",
    "            # train_tgt_text = dataset.map(lambda src, tgt: tgt)\n",
    "            # print(\"Creating vocabulary for training source texts...\")\n",
    "            # self.src_text_vectorizer.adapt(train_src_text)\n",
    "            # print(\"Creating vocabulary for training target texts...\")\n",
    "            # self.tgt_text_vectorizer.adapt(train_tgt_text)\n",
    "            # dataset = dataset.map(self.create_integer_ds)\n",
    "            print(\"Creating vocabulary for training source and target texts...\")\n",
    "            self.src_text_vectorizer.adapt(src_lines)\n",
    "            self.tgt_text_vectorizer.adapt(tgt_lines)\n",
    "            print(\"Mapping texts into integer repsentations...\")\n",
    "            src_integers, tgt_integers = self.create_integer_ds(src_lines, tgt_lines)\n",
    "            # print(src_integers.shape)\n",
    "            # print(labels.shape)\n",
    "            # dataset = tf.data.Dataset.from_tensor_slices(({\"input_1\": src_integers, \"input_2\": tgt_integers},\n",
    "            #                                               labels)).batch(batch_size)\n",
    "\n",
    "        elif mode == \"test\":\n",
    "\n",
    "            print(\"Mapping texts into integer repsentations...\")\n",
    "            src_integers, tgt_integers = self.create_integer_ds(src_lines, tgt_lines)\n",
    "            # dataset = tf.data.Dataset.from_tensor_slices(({\"input_1\": src_integers, \"input_2\": tgt_integers},\n",
    "            #                                               labels)).batch(batch_size)\n",
    "            # test_ds = tf.data.Dataset.from_tensor_slices(([src_lines, tgt_lines], labels)).batch(batch_size)\n",
    "            # dataset = dataset.map(self.create_integer_ds)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Please select mode between 'train' and 'test'.\")\n",
    "\n",
    "        return src_integers, tgt_integers, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from model import TQC_Model\n",
    "# from preprocessing import TextPreprocess\n",
    "\n",
    "\n",
    "def train(model, train_ds, train_labels, epochs, optimizer=\"adam\", **hparams):\n",
    "\n",
    "    model.compile(loss=\"binary_crossentropy\",\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=[\"accuracy\"])\n",
    "    model.fit(train_ds, train_labels, epochs=epochs)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# hyper-parameters\n",
    "srcLang = \"eng\"\n",
    "tgtLang = \"fra\"\n",
    "src_vocab_size = 20000\n",
    "src_len = 150\n",
    "tgt_vocab_size = 20000\n",
    "tgt_len = 150\n",
    "\n",
    "num_layers = 6  # the number of encoder layer for both source and target\n",
    "d_model = 128   # dimension of word for both source and target\n",
    "num_heads = 8   # the number of heads for both source and target\n",
    "dff = 2048\n",
    "maximum_position_encoding = 10000\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 3\n",
    "optimizer = \"adam\"\n",
    "\n",
    "label_class_map = {\"good\": 1, \"bad\": 0}\n",
    "\n",
    "rootpath = os.path.abspath(\"..\")\n",
    "train_data_dir = os.path.join(rootpath, \"tqa/train\")\n",
    "test_data_dir = os.path.join(rootpath, \"tqa/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Reading and preprocessing data.\n",
      "\n",
      "Importing Data\n",
      "\t1 pairs of good English-French files found.\n",
      "\t1 pairs of bad English-French files found.\n",
      "Importing Data Complete.\n",
      "\t335073 good entries\n",
      "\t128875 bad entries\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-cf7408c20ffc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m tp = TextPreprocess(src_vocab_size=src_vocab_size, src_len=src_len,\n\u001b[1;32m     10\u001b[0m                     tgt_vocab_size=tgt_vocab_size, tgt_len=tgt_len)\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0msrc_integers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_integers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_class_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mtest_src_integers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_tgt_integers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_class_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-6cb6c12f750c>\u001b[0m in \u001b[0;36mcreate_datasets\u001b[0;34m(self, data_dir, label_class_map, mode, batch_size)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;34m\"\"\"Create datasets used for training and testing from local files\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0msrc_lines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_lines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_dataset_from_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_class_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;31m# labels = tf.data.Dataset.from_tensor_slices((labels)).batch(batch_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-6cb6c12f750c>\u001b[0m in \u001b[0;36mread_dataset_from_directory\u001b[0;34m(self, data_dir, label_class_map, shuffle)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0msrc_lines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_lines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_lines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mtgt_lines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_lines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_lines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msrc_lines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_lines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "# get data ready\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"Reading and preprocessing data.\")\n",
    "\n",
    "# src_lines = np.array(src_lines)\n",
    "# tgt_lines = np.array(tgt_lines)\n",
    "# labels = np.array(labels)\n",
    "\n",
    "tp = TextPreprocess(src_vocab_size=src_vocab_size, src_len=src_len,\n",
    "                    tgt_vocab_size=tgt_vocab_size, tgt_len=tgt_len)\n",
    "src_integers, tgt_integers, labels = tp.create_datasets(train_data_dir, label_class_map, mode='train')\n",
    "test_src_integers, test_tgt_integers, test_labels = tp.create_datasets(test_data_dir, label_class_map, mode='test')\n",
    "labels = np.array(labels)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(463948, 150)\n",
      "(463948, 150)\n",
      "(463948,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(src_integers.shape)\n",
    "print(tgt_integers.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Initializing and training model.\n",
      "Epoch 1/3\n",
      "   1/3624 [..............................] - ETA: 25:44:33 - loss: 2.1272 - accuracy: 0.3125"
     ]
    }
   ],
   "source": [
    "# get model and start training\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"Initializing and training model.\")\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "with strategy.scope():\n",
    "    \n",
    "    # Everything that creates variables should be under the strategy scope.\n",
    "    # In general this is only model construction & `compile()`.\n",
    "    model = TQC_Model((src_len), (tgt_len),\n",
    "                  num_layers, d_model, num_heads, dff,\n",
    "                  src_vocab_size, tgt_vocab_size, maximum_position_encoding)\n",
    "    \n",
    "    model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "steps_per_epoch = int(src_integers.shape[0] / batch_size)\n",
    "\n",
    "model.fit(x=[src_integers, tgt_integers], \n",
    "          y=labels, \n",
    "          validation_split=0.1, \n",
    "          epochs=epochs,\n",
    "          batch_size=batch_size,\n",
    "          steps_per_epoch=steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# c = []\n",
    "# gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "\n",
    "# with tf.device(\"/gpu:0\"):\n",
    "#     a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3])\n",
    "#     b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2])\n",
    "#     c.append(tf.matmul(a, b))\n",
    "# with tf.device('/cpu:0'):\n",
    "#     sum = tf.add_n(c)\n",
    "# # Creates a session with log_device_placement set to True.\n",
    "# sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "# # Runs the op.\n",
    "# print(sess.run(sum))\n",
    "\n",
    "# from tensorflow.python.client import device_lib\n",
    "# print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "Number of devices: 1\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
