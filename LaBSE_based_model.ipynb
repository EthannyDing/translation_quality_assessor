{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as text\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D, Dense, Bidirectional, LSTM, GlobalAveragePooling1D, GlobalMaxPooling1D, Dropout, concatenate\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, recall_score, precision_score, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Importing Data\n",
      "\t5 pairs of good English-French files found.\n",
      "\t9 pairs of bad English-French files found.\n",
      "Importing Data Complete.\n",
      "\t352872 good entries\n",
      "\t417208 bad entries\n",
      "\n",
      "Importing Data\n",
      "\t1 pairs of good English-French files found.\n",
      "\t1 pairs of bad English-French files found.\n",
      "Importing Data Complete.\n",
      "\t400 good entries\n",
      "\t400 bad entries\n",
      "\n",
      "Importing Data\n",
      "\t1 pairs of good English-French files found.\n",
      "\t1 pairs of bad English-French files found.\n",
      "Importing Data Complete.\n",
      "\t400 good entries\n",
      "\t400 bad entries\n",
      "Final number of training samples: 765175\n"
     ]
    }
   ],
   "source": [
    "from preprocessing import TextPreprocessOld\n",
    "\n",
    "srcLang = \"eng\"\n",
    "tgtLang = \"fra\"\n",
    "src_vocab_size = 20000\n",
    "src_len = 150\n",
    "tgt_vocab_size = 20000\n",
    "tgt_len = 150\n",
    "train_data_dir = \"/linguistics/ethan/DL_Prototype/datasets/TQA/train\"\n",
    "test_tb_data_dir = \"/linguistics/ethan/DL_Prototype/datasets/TQA/test/TB_test\"\n",
    "test_tm_data_dir = \"/linguistics/ethan/DL_Prototype/datasets/TQA/test/TM_test\"\n",
    "\n",
    "label_class_map = {\"good\": 1, \"bad\": 0}\n",
    "tp = TextPreprocessOld(srcLang, tgtLang, src_vocab_size=src_vocab_size, src_len=src_len,\n",
    "                    tgt_vocab_size=tgt_vocab_size, tgt_len=tgt_len)\n",
    "train_src_integers, train_tgt_integers, train_labels = tp.read_dataset_from_directory(train_data_dir, label_class_map)\n",
    "test_tb_src_integers, test_tb_tgt_integers, test_tb_labels = tp.read_dataset_from_directory(test_tb_data_dir, label_class_map)\n",
    "test_tm_src_integers, test_tm_tgt_integers, test_tm_labels = tp.read_dataset_from_directory(test_tm_data_dir, label_class_map)\n",
    "\n",
    "# remove samples in training data that could exist in test data.\n",
    "test_data = set(zip(test_tm_src_integers, test_tm_tgt_integers, test_tm_labels)).union(\n",
    "            set(zip(test_tb_src_integers, test_tb_tgt_integers, test_tb_labels)))\n",
    "train_data = set(zip(train_src_integers, train_tgt_integers, train_labels))\n",
    "train_data = train_data.difference(test_data)\n",
    "\n",
    "train_src_integers = np.array([td[0] for td in train_data])\n",
    "train_tgt_integers = np.array([td[1] for td in train_data])\n",
    "train_labels = np.array([td[2] for td in train_data])\n",
    "\n",
    "print(\"Final number of training samples: {}\".format(train_src_integers.shape[0]))\n",
    "del train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Onehot encode label data if we are to use softmax activation and multiple nodes as output layer.\"\"\"\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels, num_classes=2)\n",
    "test_tm_labels = tf.keras.utils.to_categorical(test_tm_labels, num_classes=2)\n",
    "test_tb_labels = tf.keras.utils.to_categorical(test_tb_labels, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor_dir = \"/linguistics/ethan/DL_Prototype/models/universal-sentence-encoder-cmlm_multilingual-preprocess_2\"\n",
    "LaBSE_dir = \"/linguistics/ethan/DL_Prototype/models/LaBSE2_encoder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample 100 good labeled and 100 bad labeled TM\n",
    "import pandas as pd\n",
    "output = \"/linguistics/ethan/DL_Prototype/datasets/train_sample/bad_tm.sample.xlsx\"\n",
    "good_tm = []\n",
    "bad_tm = []\n",
    "for src, tgt, label in zip(train_src_integers, train_tgt_integers, train_labels):\n",
    "    if len(bad_tm) == 100:\n",
    "        break\n",
    "    if label == 0:\n",
    "        bad_tm.append((src, tgt))\n",
    "df = pd.DataFrame(bad_tm, columns=[\"English\", \"French\"])\n",
    "df.to_excel(output, header=True, index=None)\n",
    "# train_tgt_integers[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if test data has TM which also exists in training data\n",
    "# test_data = set(zip(test_tm_src_integers, test_tm_tgt_integers, test_tm_labels))\n",
    "# train_data = set(zip(train_src_integers, train_tgt_integers, train_labels))\n",
    "# train_data = train_data.difference(test_data)\n",
    "# next(iter(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intersect_sources = [p[0] for p in intersect_data]\n",
    "# train_src_integers, train_tgt_integers, train_labels\n",
    "# np.where(test_tm_src_integers in intersect_sources, True, False)\n",
    "# a = np.array([\"1\",\"2\",\"3\",\"4\"])\n",
    "# b = np.array([\"1\",\"2\",\"5\",\"4\", \"6\"])\n",
    "# (np.in1d(a, b))\n",
    "# sum(np.in1d(test_tm_src_integers, train_src_integers))\n",
    "# new_train_src_integers, new_train_tgt_integers, new_train_labels = [], [], []\n",
    "# for s, t, l in zip(train_src_integers, train_tgt_integers, train_labels):\n",
    "#     if (s not in test_tm_src_integers) and (t not in test_tm_tgt_integers):\n",
    "#         new_train_src_integers.append(s)\n",
    "#         new_train_tgt_integers.append(t)\n",
    "#         new_train_labels.append(l)\n",
    "# len(set(zip(train_src_integers, train_tgt_integers, train_labels)))\n",
    "# train_src_integers.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text vectorization using pretrained preprocessor.\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    preprocessor = hub.KerasLayer(preprocessor_dir, trainable=False)\n",
    "    \n",
    "    train_src_integers = preprocessor(train_src_integers)\n",
    "    train_tgt_integers = preprocessor(train_tgt_integers)\n",
    "\n",
    "    test_src_integers = preprocessor(test_src_integers)\n",
    "    test_tgt_integers = preprocessor(test_tgt_integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_src_integers[\"input_type_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = hub.KerasLayer(LaBSE_dir, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Define TQC models with and without preprocessor layer as part of model.\"\"\"\n",
    "\n",
    "def normalize(embeds):\n",
    "    l2_norm = np.linalg.norm(embeds, 2, axis=1, keepdims=True)\n",
    "    return embeds / l2_norm\n",
    "\n",
    "def build_model(max_seq_len):\n",
    "\n",
    "    # preprocessor = hub.KerasLayer(\n",
    "    #     \"https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-preprocess/2\")\n",
    "    # encoder = hub.KerasLayer(\"https://tfhub.dev/google/LaBSE/2\", trainable=False)\n",
    "    \n",
    "    src_word_ids = Input((max_seq_len,), dtype=tf.int32)\n",
    "    src_mask_ids = Input((max_seq_len,), dtype=tf.int32)\n",
    "    src_type_ids = Input((max_seq_len,), dtype=tf.int32)\n",
    "    \n",
    "    tgt_word_ids = Input((max_seq_len,), dtype=tf.int32)\n",
    "    tgt_mask_ids = Input((max_seq_len,), dtype=tf.int32)\n",
    "    tgt_type_ids = Input((max_seq_len,), dtype=tf.int32)\n",
    "    \n",
    "    src_x = {\"input_word_ids\": src_word_ids,\n",
    "             \"input_mask\": src_mask_ids,\n",
    "             \"input_type_ids\": src_type_ids}\n",
    "    \n",
    "    tgt_x = {\"input_word_ids\": tgt_word_ids,\n",
    "             \"input_mask\": tgt_mask_ids,\n",
    "             \"input_type_ids\": tgt_type_ids}\n",
    "    \n",
    "    src_x = encoder(src_x)[\"default\"]\n",
    "    tgt_x = encoder(tgt_x)[\"default\"]\n",
    "    \n",
    "    src_x = tf.math.l2_normalize(src_x, axis=1, epsilon=1e-12, name=None)\n",
    "    tgt_x = tf.math.l2_normalize(tgt_x, axis=1, epsilon=1e-12, name=None)\n",
    "    \n",
    "    # np.matmul(english_embeds, np.transpose(italian_embeds))\n",
    "    x = tf.concat([src_x, tgt_x], axis=1)\n",
    "    #  x = GlobalMaxPooling1D(x)\n",
    "    \n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    output = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model([src_word_ids, src_mask_ids, src_type_ids, \n",
    "               tgt_word_ids, tgt_mask_ids, tgt_type_ids], output)\n",
    "    #  model = Model([src_x, tgt_x], output)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def build_model_with_preprocessor(max_seq_len, preprocessor_dir, LaBSE_dir):\n",
    "    \n",
    "    src_texts = tf.keras.layers.Input(shape=(), dtype=tf.string, name=\"input_src_text\")\n",
    "    tgt_texts = tf.keras.layers.Input(shape=(), dtype=tf.string, name=\"input_tgt_text\")\n",
    "\n",
    "    preprocessor = hub.KerasLayer(preprocessor_dir, trainable=False)\n",
    "    encoder = hub.KerasLayer(LaBSE_dir, trainable=False)\n",
    "    \n",
    "    src_x = preprocessor(src_texts)\n",
    "    tgt_x = preprocessor(tgt_texts)\n",
    "    \n",
    "    src_x = encoder(src_x)[\"default\"]\n",
    "    tgt_x = encoder(tgt_x)[\"default\"]\n",
    "    \n",
    "    src_x = tf.math.l2_normalize(src_x, axis=1, epsilon=1e-12, name=None)\n",
    "    tgt_x = tf.math.l2_normalize(tgt_x, axis=1, epsilon=1e-12, name=None)\n",
    "    \n",
    "    # np.matmul(english_embeds, np.transpose(italian_embeds))\n",
    "    x = tf.concat([src_x, tgt_x], axis=1)\n",
    "    #  x = GlobalMaxPooling1D(x)\n",
    "\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dense(8, activation='relu')(x)\n",
    "    output = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model([src_texts, tgt_texts], output)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def build_model_with_preprocessor_and_lstm(max_seq_len, \n",
    "                                           preprocessor_dir, \n",
    "                                           LaBSE_dir, \n",
    "                                           softmax=False,\n",
    "                                           preprocessor_trainable=False,\n",
    "                                           encoder_trainable=False\n",
    "                                          ):\n",
    "    \"\"\"Once softmax output layer is turned on, make sure to onehot encode labeled data to shape (n, num_classes)\"\"\"\n",
    "    \n",
    "    src_texts = tf.keras.layers.Input(shape=(), dtype=tf.string, name=\"input_src_text\")\n",
    "    tgt_texts = tf.keras.layers.Input(shape=(), dtype=tf.string, name=\"input_tgt_text\")\n",
    "\n",
    "    preprocessor = hub.KerasLayer(preprocessor_dir, trainable=preprocessor_trainable)\n",
    "    encoder = hub.KerasLayer(LaBSE_dir, trainable=encoder_trainable)\n",
    "    \n",
    "    src_x = preprocessor(src_texts)\n",
    "    tgt_x = preprocessor(tgt_texts)\n",
    "    \n",
    "    src_x = encoder(src_x)[\"sequence_output\"]\n",
    "    tgt_x = encoder(tgt_x)[\"sequence_output\"]\n",
    "    \n",
    "    # src_x = tf.math.l2_normalize(src_x, axis=-1, epsilon=1e-12, name=None)\n",
    "    # tgt_x = tf.math.l2_normalize(tgt_x, axis=-1, epsilon=1e-12, name=None)\n",
    "    \n",
    "    # np.matmul(english_embeds, np.transpose(italian_embeds))\n",
    "    # sequence_output = tf.concat([src_x, tgt_x], axis=-1)\n",
    "    sequence_output = concatenate([src_x, tgt_x])\n",
    "    \n",
    "    # Add trainable layers on top of frozen layers to adapt the pretrained features on the new data.\n",
    "    bi_lstm = Bidirectional(LSTM(768, return_sequences=True))(sequence_output)\n",
    "    # Applying hybrid pooling approach to bi_lstm sequence output.\n",
    "    avg_pool = GlobalAveragePooling1D()(bi_lstm)\n",
    "    max_pool = GlobalMaxPooling1D()(bi_lstm)\n",
    "    concat = concatenate([avg_pool, max_pool])\n",
    "    dropout = Dropout(0.3)(concat)\n",
    "    \n",
    "    x = Dense(2048, activation=\"relu\")(dropout)\n",
    "    x = Dense(512, activation=\"relu\")(x)\n",
    "    x = Dense(8, activation=\"relu\")(x)\n",
    "    \n",
    "    if softmax:\n",
    "        output = Dense(2, activation='softmax')(x)\n",
    "        \n",
    "    else:\n",
    "        output = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    \n",
    "    model = Model([src_texts, tgt_texts], output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeds = tf.constant([[1,2,3,4], [1,2, 3, 4]], dtype=tf.float32)\n",
    "# embeds / np.linalg.norm(embeds, 2, axis=1, keepdims=True)\n",
    "# # embeds\n",
    "# tf.math.l2_normalize(\n",
    "#     embeds, axis=1, epsilon=1e-12, name=None\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Input((128,))\n",
    "output = Dense(2, activation='softmax')(x)\n",
    "# output = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(x, output)\n",
    "\n",
    "model.summary()\n",
    "x = tf.random.uniform((1,128))\n",
    "model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 / np.sqrt(30)\n",
    "# tf.keras.layers.Lambda(\n",
    "#       lambda x: tf.nn.l2_normalize(x, axis=1))(embeds)\n",
    "# tf.nn.l2_normalize(embeds, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Specify training hyperparameters\"\"\"\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "gpu_devices = get_available_gpus()\n",
    "epochs = 10\n",
    "max_seq_len = 128\n",
    "batch_size = 128\n",
    "num_samples = len(train_labels)\n",
    "steps_per_epoch = int(num_samples / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = build_model(max_seq_len)\n",
    "# model2 = build_model_with_preprocessor(max_seq_len, preprocessor_dir, LaBSE_dir)\n",
    "model3 = build_model_with_preprocessor_and_lstm(max_seq_len, preprocessor_dir, LaBSE_dir, softmax=True)\n",
    "# del model3\n",
    "# model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model2.summary()\n",
    "# mirrored_strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])\n",
    "# with mirrored_strategy.scope():\n",
    "# for d in [\"/gpu:0\", \"/gpu:1\"]:\n",
    "    \n",
    "#     with tf.device(d):\n",
    "\n",
    "#         model = build_model(max_seq_len)\n",
    "#         model.compile(optimizer=\"adam\",\n",
    "#                       loss=\"binary_crossentropy\",\n",
    "#                       metrics=[\"accuracy\"])\n",
    "#         model.fit(x=[train_src_integers[\"input_word_ids\"], train_src_integers[\"input_mask\"], train_src_integers[\"input_type_ids\"],\n",
    "#                      train_tgt_integers[\"input_word_ids\"], train_tgt_integers[\"input_mask\"], train_tgt_integers[\"input_type_ids\"]],\n",
    "#                   y=train_labels,\n",
    "#                   batch_size=batch_size,\n",
    "#                   epochs=epochs,\n",
    "#                   steps_per_epoch=steps_per_epoch,\n",
    "#                   validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "Epoch 1/10\n",
      "INFO:tensorflow:batch_all_reduce: 212 all-reduces with algorithm = nccl, num_packs = 1\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 1 IndexedSlices\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "INFO:tensorflow:Error reported to Coordinator: OOM when allocating tensor with shape[501153,768] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Fill]\n",
      "Traceback (most recent call last):\n",
      "  File \"/linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\n",
      "    yield\n",
      "  File \"/linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_run.py\", line 323, in run\n",
      "    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\n",
      "  File \"/linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 255, in wrapper\n",
      "    return converted_call(f, args, kwargs, options=options)\n",
      "  File \"/linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 532, in converted_call\n",
      "    return _call_unconverted(f, args, kwargs, options)\n",
      "  File \"/linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 339, in _call_unconverted\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 789, in run_step\n",
      "    outputs = model.train_step(data)\n",
      "  File \"/linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 757, in train_step\n",
      "    self.trainable_variables)\n",
      "  File \"/linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 2745, in _minimize\n",
      "    experimental_aggregate_gradients=False)\n",
      "  File \"/linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\", line 519, in apply_gradients\n",
      "    self._create_all_weights(var_list)\n",
      "  File \"/linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\", line 704, in _create_all_weights\n",
      "    self._create_slots(var_list)\n",
      "  File \"/linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/adam.py\", line 129, in _create_slots\n",
      "    self.add_slot(var, 'v')\n",
      "  File \"/linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\", line 764, in add_slot\n",
      "    initial_value=initial_value)\n",
      "  File \"/linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 262, in __call__\n",
      "    return cls._variable_v2_call(*args, **kwargs)\n",
      "  File \"/linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 256, in _variable_v2_call\n",
      "    shape=shape)\n",
      "  File \"/linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 67, in getter\n",
      "    return captured_getter(captured_previous, **kwargs)\n",
      "  File \"/linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py\", line 2124, in create_colocated_variable\n",
      "    return next_creator(**kwargs)\n",
      "  File \"/linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 67, in getter\n",
      "    return captured_getter(captured_previous, **kwargs)\n",
      "  File \"/linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/shared_variable_creator.py\", line 69, in create_new_variable\n",
      "    v = next_creator(**kwargs)\n",
      "  File \"/linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 67, in getter\n",
      "    return captured_getter(captured_previous, **kwargs)\n",
      "  File \"/linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py\", line 2024, in creator_with_resource_vars\n",
      "    created = self._create_variable(next_creator, **kwargs)\n",
      "  File \"/linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_strategy.py\", line 450, in _create_variable\n",
      "    values.MirroredVariable, values.SyncOnReadVariable, **kwargs)\n",
      "  File \"/linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_utils.py\", line 291, in create_mirrored_variable\n",
      "    value_list = real_mirrored_creator(**kwargs)\n",
      "  File \"/linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_strategy.py\", line 443, in _real_mirrored_creator\n",
      "    v = next_creator(**kwargs)\n",
      "  File \"/linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 67, in getter\n",
      "    return captured_getter(captured_previous, **kwargs)\n",
      "  File \"/linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 685, in variable_capturing_scope\n",
      "    lifted_initializer_graph=lifted_initializer_graph, **kwds)\n",
      "  File \"/linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 264, in __call__\n",
      "    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n",
      "  File \"/linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 226, in __init__\n",
      "    initial_value() if init_from_fn else initial_value,\n",
      "  File \"/linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/initializers/initializers_v2.py\", line 137, in __call__\n",
      "    return super(Zeros, self).__call__(shape, dtype=_get_dtype(dtype))\n",
      "  File \"/linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops_v2.py\", line 132, in __call__\n",
      "    return array_ops.zeros(shape, dtype)\n",
      "  File \"/linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\n",
      "    return target(*args, **kwargs)\n",
      "  File \"/linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\", line 2747, in wrapped\n",
      "    tensor = fun(*args, **kwargs)\n",
      "  File \"/linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\", line 2806, in zeros\n",
      "    output = fill(shape, constant(zero, dtype=dtype), name=name)\n",
      "  File \"/linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\n",
      "    return target(*args, **kwargs)\n",
      "  File \"/linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\", line 239, in fill\n",
      "    result = gen_array_ops.fill(dims, value, name=name)\n",
      "  File \"/linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 3402, in fill\n",
      "    _ops.raise_from_not_ok_status(e, name)\n",
      "  File \"/linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 6843, in raise_from_not_ok_status\n",
      "    six.raise_from(core._status_to_exception(e.code, message), None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[501153,768] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Fill]\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "in user code:\n\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:806 train_function  *\n        return step_function(self, iterator)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_strategy.py:585 _call_for_each_replica\n        self._container_strategy(), fn, args, kwargs)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_run.py:96 call_for_each_replica\n        return _call_for_each_replica(strategy, fn, args, kwargs)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_run.py:237 _call_for_each_replica\n        coord.join(threads)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/coordinator.py:389 join\n        six.reraise(*self._exc_info_to_raise)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/six.py:703 reraise\n        raise value\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/coordinator.py:297 stop_on_exception\n        yield\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_run.py:323 run\n        self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:789 run_step  **\n        outputs = model.train_step(data)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:757 train_step\n        self.trainable_variables)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:2745 _minimize\n        experimental_aggregate_gradients=False)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:519 apply_gradients\n        self._create_all_weights(var_list)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:704 _create_all_weights\n        self._create_slots(var_list)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/adam.py:129 _create_slots\n        self.add_slot(var, 'v')\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:764 add_slot\n        initial_value=initial_value)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variables.py:262 __call__\n        return cls._variable_v2_call(*args, **kwargs)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variables.py:256 _variable_v2_call\n        shape=shape)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variables.py:67 getter\n        return captured_getter(captured_previous, **kwargs)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2124 create_colocated_variable\n        return next_creator(**kwargs)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variables.py:67 getter\n        return captured_getter(captured_previous, **kwargs)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/shared_variable_creator.py:69 create_new_variable\n        v = next_creator(**kwargs)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variables.py:67 getter\n        return captured_getter(captured_previous, **kwargs)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2024 creator_with_resource_vars\n        created = self._create_variable(next_creator, **kwargs)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_strategy.py:450 _create_variable\n        values.MirroredVariable, values.SyncOnReadVariable, **kwargs)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_utils.py:291 create_mirrored_variable\n        value_list = real_mirrored_creator(**kwargs)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_strategy.py:443 _real_mirrored_creator\n        v = next_creator(**kwargs)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variables.py:67 getter\n        return captured_getter(captured_previous, **kwargs)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py:685 variable_capturing_scope\n        lifted_initializer_graph=lifted_initializer_graph, **kwds)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variables.py:264 __call__\n        return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py:226 __init__\n        initial_value() if init_from_fn else initial_value,\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/initializers/initializers_v2.py:137 __call__\n        return super(Zeros, self).__call__(shape, dtype=_get_dtype(dtype))\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops_v2.py:132 __call__\n        return array_ops.zeros(shape, dtype)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:2747 wrapped\n        tensor = fun(*args, **kwargs)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:2806 zeros\n        output = fill(shape, constant(zero, dtype=dtype), name=name)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:239 fill\n        result = gen_array_ops.fill(dims, value, name=name)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py:3402 fill\n        _ops.raise_from_not_ok_status(e, name)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:6843 raise_from_not_ok_status\n        six.raise_from(core._status_to_exception(e.code, message), None)\n    <string>:3 raise_from\n        \n\n    ResourceExhaustedError: OOM when allocating tensor with shape[501153,768] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Fill]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-325b3defe40b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m                \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                \u001b[0;31m#  class_weight=class_weight,  # enable sensitive learning if dataset is unbalanced.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                validation_split=0.1)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    821\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    824\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    696\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 697\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2854\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2855\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2856\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3213\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3215\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3073\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3074\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3075\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3076\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3077\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    971\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: in user code:\n\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:806 train_function  *\n        return step_function(self, iterator)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_strategy.py:585 _call_for_each_replica\n        self._container_strategy(), fn, args, kwargs)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_run.py:96 call_for_each_replica\n        return _call_for_each_replica(strategy, fn, args, kwargs)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_run.py:237 _call_for_each_replica\n        coord.join(threads)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/coordinator.py:389 join\n        six.reraise(*self._exc_info_to_raise)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/six.py:703 reraise\n        raise value\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/coordinator.py:297 stop_on_exception\n        yield\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_run.py:323 run\n        self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:789 run_step  **\n        outputs = model.train_step(data)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:757 train_step\n        self.trainable_variables)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:2745 _minimize\n        experimental_aggregate_gradients=False)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:519 apply_gradients\n        self._create_all_weights(var_list)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:704 _create_all_weights\n        self._create_slots(var_list)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/adam.py:129 _create_slots\n        self.add_slot(var, 'v')\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:764 add_slot\n        initial_value=initial_value)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variables.py:262 __call__\n        return cls._variable_v2_call(*args, **kwargs)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variables.py:256 _variable_v2_call\n        shape=shape)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variables.py:67 getter\n        return captured_getter(captured_previous, **kwargs)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2124 create_colocated_variable\n        return next_creator(**kwargs)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variables.py:67 getter\n        return captured_getter(captured_previous, **kwargs)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/shared_variable_creator.py:69 create_new_variable\n        v = next_creator(**kwargs)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variables.py:67 getter\n        return captured_getter(captured_previous, **kwargs)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2024 creator_with_resource_vars\n        created = self._create_variable(next_creator, **kwargs)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_strategy.py:450 _create_variable\n        values.MirroredVariable, values.SyncOnReadVariable, **kwargs)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_utils.py:291 create_mirrored_variable\n        value_list = real_mirrored_creator(**kwargs)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_strategy.py:443 _real_mirrored_creator\n        v = next_creator(**kwargs)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variables.py:67 getter\n        return captured_getter(captured_previous, **kwargs)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py:685 variable_capturing_scope\n        lifted_initializer_graph=lifted_initializer_graph, **kwds)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variables.py:264 __call__\n        return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py:226 __init__\n        initial_value() if init_from_fn else initial_value,\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/initializers/initializers_v2.py:137 __call__\n        return super(Zeros, self).__call__(shape, dtype=_get_dtype(dtype))\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops_v2.py:132 __call__\n        return array_ops.zeros(shape, dtype)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:2747 wrapped\n        tensor = fun(*args, **kwargs)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:2806 zeros\n        output = fill(shape, constant(zero, dtype=dtype), name=name)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:239 fill\n        result = gen_array_ops.fill(dims, value, name=name)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py:3402 fill\n        _ops.raise_from_not_ok_status(e, name)\n    /linguistics/ethan/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:6843 raise_from_not_ok_status\n        six.raise_from(core._status_to_exception(e.code, message), None)\n    <string>:3 raise_from\n        \n\n    ResourceExhaustedError: OOM when allocating tensor with shape[501153,768] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Fill]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Start training session.\"\"\"\n",
    "\n",
    "# Specify checkpoint saving every epoch\n",
    "checkpoint_path = \"/linguistics/ethan/DL_Prototype/models/TQA_models/Multilingual-LaBSE-Bidirectional-LSTM_ckpts/training_job_8/tqc-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, \n",
    "                                              verbose=1,\n",
    "                                              save_weights_only=False,\n",
    "                                              monitor=\"val_accuracy\",\n",
    "                                              mode=\"max\",\n",
    "                                              save_best_only=True)\n",
    "                                              #  save_freq='epoch') # Save weights, every epoch.\n",
    "\n",
    "# Specify weights for each class, especially for imbalanced datasets.\n",
    "# weights = compute_class_weight(\"balanced\", np.unique(train_labels), train_labels)\n",
    "# class_weight = dict(zip(np.unique(train_labels), weights))\n",
    "\n",
    "# specify metrics during and after training.\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])\n",
    "with mirrored_strategy.scope():\n",
    "    #  model = build_model(max_seq_len)\n",
    "    #  model2 = build_model_with_preprocessor(max_seq_len, preprocessor_dir, LaBSE_dir)\n",
    "    model3 = build_model_with_preprocessor_and_lstm(max_seq_len, preprocessor_dir, LaBSE_dir, softmax=False, encoder_trainable=True)\n",
    "    model3.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                  loss=\"binary_crossentropy\",  # categorical_crossentropy if label is (n, num_classes), othervise binary_crissentropy or sparse_categorical_crossentropy\n",
    "                  metrics=[\"accuracy\", recall_m, precision_m]\n",
    "                  )\n",
    "\n",
    "    model3.fit(x=[train_src_integers, train_tgt_integers],\n",
    "               y=train_labels,\n",
    "               batch_size=batch_size,\n",
    "               epochs=epochs,\n",
    "               steps_per_epoch=steps_per_epoch,\n",
    "               callbacks=[callback],\n",
    "               #  class_weight=class_weight,  # enable sensitive learning if dataset is unbalanced.\n",
    "               validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_tm_src_integers.shape)\n",
    "print(test_tm_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evalution on Test data.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions from trained model on test inputs.\n",
    "# predictions = model.predict([test_src_integers[\"input_word_ids\"], test_src_integers[\"input_mask\"], test_src_integers[\"input_type_ids\"],\n",
    "#                              test_tgt_integers[\"input_word_ids\"], test_tgt_integers[\"input_mask\"], test_tgt_integers[\"input_type_ids\"]])\n",
    "tm_predictions = model3.predict([test_tm_src_integers, test_tm_tgt_integers])\n",
    "tb_predictions = model3.predict([test_tb_src_integers, test_tb_tgt_integers])\n",
    "\n",
    "tm_pred = [1 if p > 0.5 else 0 for p in tm_predictions]\n",
    "tb_pred = [1 if p > 0.5 else 0 for p in tb_predictions]\n",
    "\n",
    "print(confusion_matrix(test_tm_labels, tm_pred, labels=[1,0]))\n",
    "print(confusion_matrix(test_tb_labels, tb_pred, labels=[1,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.argmax(tm_predictions, axis=1))\n",
    "# print(tm_predictions)\n",
    "# print(confusion_matrix(test_tm_labels, np.argmax(tm_predictions, axis=1), labels=[1,0]))\n",
    "# print(confusion_matrix(test_tb_labels, np.argmax(tb_predictions, axis=1), labels=[1,0]))\n",
    "\n",
    "true_labels = test_tm_labels\n",
    "pred = np.argmax(tm_predictions, axis=1)\n",
    "\n",
    "print(accuracy_score(true_labels, pred))\n",
    "print(recall_score(true_labels, pred, labels=[1, 0]))\n",
    "print(precision_score(true_labels, pred, labels=[1, 0]))\n",
    "print(f1_score(true_labels, pred, labels=[1, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# threshold = 0.3\n",
    "def evaluate(predictions, true_labels):\n",
    "    records = []\n",
    "    for threshold in np.linspace(0.1, 0.9, num=9):\n",
    "        pred = [1 if p > threshold else 0 for p in predictions]\n",
    "        acc = accuracy_score(true_labels, pred)\n",
    "        rec = recall_score(true_labels, pred, labels=[1, 0])\n",
    "        pre = precision_score(true_labels, pred, labels=[1, 0])\n",
    "        f1 = f1_score(true_labels, pred, labels=[1, 0])\n",
    "        records.append((threshold, acc, rec, pre, f1))\n",
    "\n",
    "    df = pd.DataFrame(records, columns=[\"Threshold\", \"Accuracy\", \"Recall\", \"Precision\", \"F1\"])\n",
    "    return df\n",
    "\n",
    "evaluate(tm_predictions, test_tm_labels)\n",
    "# evaluate(tb_predictions, test_tb_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_model_dir = \"/linguistics/ethan/DL_Prototype/models/LaBSE2_based_tqc\"\n",
    "model.save(cls_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Load complete trained model.\"\"\"\n",
    "\n",
    "# Load trained model weights for defined model.\n",
    "# cls_weights_dir = \"/linguistics/ethan/DL_Prototype/models/TQA_models/Multilingual-LaBSE-Bidirectional-LSTM_ckpts/training_job_7/tqc-0005.ckpt\"\n",
    "# model3.load_weights(cls_weights_dir)\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "custom_objects={\"recall_m\": recall_m, \"precision_m\": precision_m}\n",
    "cls_model_dir = \"/linguistics/ethan/DL_Prototype/models/TQA_models/Multilingual-LaBSE-Bidirectional-LSTM_ckpts/training_job_7/tqc-0008.ckpt\"\n",
    "model3 = tf.keras.models.load_model(cls_model_dir, custom_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction = model3.predict()\n",
    "# output = \n",
    "# new_model3 = tf.keras.Model(model3.inputs,\n",
    "# dir(model3)\n",
    "# input_array = np.array([[1,2], [3,4]])\n",
    "# print(input_array.shape)\n",
    "# dense_output = Dense(1, activation='sigmoid')(input_array)\n",
    "# print(dense_output) \n",
    "# tf.where(dense_output > 0.5, 1, 0)\n",
    "# dense_output = np.array([[0.2], [0.4], [0.6]])\n",
    "# np.where(dense_output > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "accuracy_fn = tf.keras.metrics.BinaryAccuracy()\n",
    "logits = np.array([[0.4, 0.6], [0.2, 0.8]])\n",
    "targets = np.array([[1, 0], [0, 1]])\n",
    "print(loss_fn(targets, logits, None))\n",
    "print(accuracy_fn(targets, logits, None))\n",
    "print(tf.nn.softmax(logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify1(text_pairs, threshold=0.5):\n",
    "    \n",
    "    src_integers = preprocessor([p[0] for p in text_pairs])\n",
    "    tgt_integers = preprocessor([p[1] for p in text_pairs])\n",
    "    \n",
    "    predictions = model.predict([src_integers[\"input_word_ids\"], src_integers[\"input_mask\"], src_integers[\"input_type_ids\"],\n",
    "                                 tgt_integers[\"input_word_ids\"], tgt_integers[\"input_mask\"], tgt_integers[\"input_type_ids\"]])\n",
    "    \n",
    "    return [1 if p > threshold else 0 for p in predictions]\n",
    "\n",
    "def classify2(text_pairs, threshold=0.5):\n",
    "    \n",
    "    return [1 if p > threshold else 0 for p in model3.predict(text_pairs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pairs = [(\"Poly Property Group Company Limited\", \n",
    "               \"Poly Property Group Company Limited\"),\n",
    "              \n",
    "              (\"Amazon also indicated that it has moved its AI plans from hype to reality.\", \n",
    "               \"Amazon a aussi indiqu que ses projets dIA, jusqualors des mythes, taient devenus ralit.\"),\n",
    "              \n",
    "              (\"A regulation may not be made before the earliest of\", \n",
    "               \"Le rglement ne peut tre pris avant le premier en date des jours suivants\"),\n",
    "             \n",
    "              (\"Nancy J. Kyle is a vice chairman and director of CGTC.\",\n",
    "               \"Nancy J. Kyle est vice-prsidente du conseil dadministration et\")]\n",
    "\n",
    "classify(text_pairs, threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_texts = tf.constant([ \"Poly Property Group Company Limited\",\n",
    "                          \"Amazon also indicated that it has moved its AI plans from hype to reality.\", \n",
    "                          \"A regulation may not be made before the earliest of\",\n",
    "                          \"Nancy J. Kyle is a vice chairman and director of CGTC.\",\n",
    "                          \"You can copy-paste\"])\n",
    "\n",
    "tgt_texts = tf.constant([\"Poly Property Group Company Limited\", \n",
    "                         \"Amazon a aussi indiqu que ses projets dIA, jusqualors des mythes, taient devenus ralit.\",\n",
    "                         \"Le rglement ne peut tre pris avant le premier en date des jours suivants\",\n",
    "                         \"Nancy J. Kyle est vice-prsidente du conseil dadministration et\",\n",
    "                         \"Il est possible de copier-coller\"])\n",
    "\n",
    "[\"Poly Property Group Company Limited\", \"Amazon a aussi indiqu que ses projets dIA, jusqualors des mythes, taient devenus ralit.\", \"Le rglement ne peut tre pris avant le premier en date des jours suivants\", \"Nancy J. Kyle est vice-prsidente du conseil dadministration et\", \"Il est possible de copier-coller\"]\n",
    "classify2([src_texts, tgt_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model2.predict([src_texts, tgt_texts])\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model3, \"/linguistics/ethan/DL_Prototype/models/TQA_models/Multilingual-LaBSE-Bidirectional-LSTM_ckpts/training_job_7/LaBSE_bi-LSTM_based.softmax.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
