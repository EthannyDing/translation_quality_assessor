{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from model import TQC_Model\n",
    "from preprocessing import TextPreprocess\n",
    "\n",
    "\n",
    "def train(model, train_ds, train_labels, epochs, optimizer=\"adam\", **hparams):\n",
    "\n",
    "    model.compile(loss=\"binary_crossentropy\",\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=[\"accuracy\"])\n",
    "    model.fit(train_ds, train_labels, epochs=epochs)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# hyper-parameters\n",
    "srcLang = \"eng\"\n",
    "tgtLang = \"fra\"\n",
    "src_vocab_size = 20000\n",
    "src_len = 150\n",
    "tgt_vocab_size = 20000\n",
    "tgt_len = 150\n",
    "\n",
    "num_layers = 6  # the number of encoder layer for both source and target\n",
    "d_model = 128   # dimension of word for both source and target\n",
    "num_heads = 8   # the number of heads for both source and target\n",
    "dff = 2048\n",
    "maximum_position_encoding = 10000\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 3\n",
    "optimizer = \"adam\"\n",
    "\n",
    "label_class_map = {\"good\": 1, \"bad\": 0}\n",
    "\n",
    "rootpath = os.path.abspath(\"..\")\n",
    "train_data_dir = os.path.join(rootpath, \"datasets/tqa/train\")\n",
    "test_data_dir = os.path.join(rootpath, \"datasets/tqa/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Reading and preprocessing data.\n",
      "\n",
      "Importing Data\n",
      "\t1 pairs of good English-French files found.\n",
      "\t1 pairs of bad English-French files found.\n",
      "Importing Data Complete.\n",
      "\t335073 good entries\n",
      "\t128875 bad entries\n",
      "Creating vocabulary for training source and target texts...\n",
      "Mapping texts into integer repsentations...\n",
      "\n",
      "Importing Data\n",
      "\t1 pairs of good English-French files found.\n",
      "\t1 pairs of bad English-French files found.\n",
      "Importing Data Complete.\n",
      "\t400 good entries\n",
      "\t400 bad entries\n",
      "Mapping texts into integer repsentations...\n"
     ]
    }
   ],
   "source": [
    "# get data ready\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"Reading and preprocessing data.\")\n",
    "tp = TextPreprocess(src_vocab_size=src_vocab_size, src_len=src_len,\n",
    "                    tgt_vocab_size=tgt_vocab_size, tgt_len=tgt_len)\n",
    "src_integers, tgt_integers, labels = tp.create_datasets(train_data_dir, label_class_map, mode='train')\n",
    "test_src_integers, test_tgt_integers, test_labels = tp.create_datasets(test_data_dir, label_class_map, mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(463948, 150)\n",
      "(463948, 150)\n",
      "(463948,)\n"
     ]
    }
   ],
   "source": [
    "labels = np.array(labels)\n",
    "print(src_integers.shape)\n",
    "print(tgt_integers.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Initializing and training model.\n",
      "Epoch 1/3\n",
      "  85/3624 [..............................] - ETA: 4:34:34 - loss: 0.3511 - accuracy: 0.8643"
     ]
    }
   ],
   "source": [
    "# get model and start training\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"Initializing and training model.\")\n",
    "model = TQC_Model((src_len), (tgt_len),\n",
    "                  num_layers, d_model, num_heads, dff,\n",
    "                  src_vocab_size, tgt_vocab_size, maximum_position_encoding)\n",
    "\n",
    "steps_per_epoch = int(src_integers.shape[0] / batch_size)\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x=[src_integers, tgt_integers], \n",
    "          y=labels, \n",
    "          validation_split=0.1, \n",
    "          epochs=epochs,\n",
    "          batch_size=batch_size,\n",
    "          steps_per_epoch=steps_per_epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
